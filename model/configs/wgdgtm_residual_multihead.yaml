# WG-DGTM upgraded config: residual forecasting + multi-head decoder
seed: 42

data:
  processed_dir: "processed"
  p1_deep_dir: "processed/P1_deep"
  graphs_dir: "processed/graphs"

task:
  lookback: 168
  horizon: 24
  num_stations: 12
  num_targets: 6

model:
  # Upgrade #1: residual forecasting over persistence baseline
  use_residual_forecasting: true

  d_model: 64
  d_qk: 64
  d_node_emb: 16
  dropout: 0.1

  wind_gate:
    hidden_dim: 32
    lambda_gate: 0.5

  graph_fusion:
    alpha_init: 1.0
    beta_init: 1.0
    gamma_init: 1.0
    add_self_loops: true

  spatial:
    out_dim: 64

  tcn:
    channels: 64
    num_layers: 6
    kernel_size: 3
    dropout: 0.2

  decoder:
    # Upgrade #2: one head per pollutant
    type: "multihead"  # "shared" | "multihead"
    horizon_emb_dim: 16
    hidden_dim: 64
    dropout: 0.1

training:
  batch_size: 64
  num_workers: 0
  epochs: 50
  learning_rate: 0.001
  weight_decay: 0.0001
  grad_clip: 5.0
  patience: 8
  min_delta: 0.0
  log_interval: 50

  # Multi-GPU (optional): uses torch.nn.DataParallel when >1 GPU is visible.
  use_data_parallel: true
  gpus: null

  # Required: std-weighted masked MAE loss
  loss_eps: 1.0e-6

results:
  dir: "model/results"
  experiment_name: "wgdgtm_residual_multihead"

debug:
  assert_shapes: true

